{"cells":[{"cell_type":"markdown","id":"4139cc19-dd23-4ffa-b6a2-a1b70a6daec6","metadata":{"id":"4139cc19-dd23-4ffa-b6a2-a1b70a6daec6"},"source":["<center><h1>CSCI 4140: Natural Language Processing</h1></center>\n","<center><h1>CSCI/DASC 6040: Computational Analysis of Natural Languages</h1></center>\n","\n","<center><h6>Spring 2024</h6></center>\n","<center><h6>Homework 1 - N-gram models</h6></center>\n","<center><h6>Due Sunday, January 21, at 11:59 PM</h6></center>\n","\n","<center><font color='red'>Do not redistribute without the instructor’s written permission.</font></center>"]},{"cell_type":"markdown","id":"166d5179-89ef-4186-9af3-ce63ff95cc8b","metadata":{"id":"166d5179-89ef-4186-9af3-ce63ff95cc8b"},"source":["The learning goals of this assignment are to:\n","\n","- Understand how to compute language model probabilities using maximum likelihood estimation.\n","- Implement back-off.\n","- Have fun using a language model to probabilistically generate texts.\n","- Compare word-level langauage models and character-level language models."]},{"cell_type":"markdown","id":"40408d08-1f30-4286-be61-ac741db46121","metadata":{"id":"40408d08-1f30-4286-be61-ac741db46121"},"source":["# Part 1: N-gram Language model (60 pts)"]},{"cell_type":"markdown","id":"79f41ccf-d54b-4f5d-bd2d-4c8811cd84d1","metadata":{"id":"79f41ccf-d54b-4f5d-bd2d-4c8811cd84d1"},"source":["## Preliminaries"]},{"cell_type":"code","execution_count":123,"id":"84910e85-68eb-41ab-996f-65cad5d8071e","metadata":{"id":"84910e85-68eb-41ab-996f-65cad5d8071e","executionInfo":{"status":"ok","timestamp":1705875175625,"user_tz":300,"elapsed":263,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}}},"outputs":[],"source":["import random\n","from collections import *\n","import numpy as np"]},{"cell_type":"markdown","id":"b02bac8f-9980-4905-8029-1d48434008cd","metadata":{"id":"b02bac8f-9980-4905-8029-1d48434008cd"},"source":["We'll start by loading the data. The WikiText language modeling dataset is a collection of tokens extracted from the set of verified Good and Featured articles on Wikipedia."]},{"cell_type":"code","execution_count":124,"id":"22459dd5-ddf4-4e53-9e4a-570646a15820","metadata":{"id":"22459dd5-ddf4-4e53-9e4a-570646a15820","executionInfo":{"status":"ok","timestamp":1705875181162,"user_tz":300,"elapsed":790,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}}},"outputs":[],"source":["data = {'test': '', 'train': '', 'valid': ''}\n","\n","for data_split in data:\n","    fname = \"wiki.{}.tokens\".format(data_split)\n","    with open(fname, 'r') as f_wiki:\n","        data[data_split] = f_wiki.read().lower().split()\n","\n","vocab = list(set(data['train']))"]},{"cell_type":"markdown","id":"38de2aac-d6c2-4ec2-b0d1-5e2305949705","metadata":{"id":"38de2aac-d6c2-4ec2-b0d1-5e2305949705"},"source":["Now have a look at the data by running this cell."]},{"cell_type":"code","execution_count":125,"id":"d733449a-e810-4cd1-a03d-e309d41e1dd6","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"d733449a-e810-4cd1-a03d-e309d41e1dd6","executionInfo":{"status":"ok","timestamp":1705875182727,"user_tz":300,"elapsed":251,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}},"outputId":"35c99b9d-3b20-40df-83bf-35e3b3334db3"},"outputs":[{"output_type":"stream","name":"stdout","text":["train : ['=', 'valkyria', 'chronicles', 'iii', '=', 'senjō', 'no', 'valkyria', '3', ':'] ...\n","dev : ['=', 'homarus', 'gammarus', '=', 'homarus', 'gammarus', ',', 'known', 'as', 'the'] ...\n","test : ['=', 'robert', '<unk>', '=', 'robert', '<unk>', 'is', 'an', 'english', 'film'] ...\n","first 10 words in vocab: ['noble', 'effigies', 'kovacs', '272', 'balista', '攻殻機動隊', '2e', 'creeks', 'laverton', 'refute']\n"]}],"source":["print('train : %s ...' % data['train'][:10])\n","print('dev : %s ...' % data['valid'][:10])\n","print('test : %s ...' % data['test'][:10])\n","print('first 10 words in vocab: %s' % vocab[:10])"]},{"cell_type":"markdown","id":"872e25cc-9b86-408a-b6f0-1fb139effc34","metadata":{"id":"872e25cc-9b86-408a-b6f0-1fb139effc34"},"source":["## Q1.1: Train N-gram language model (25 pts)\n","\n","Complete the following `train_ngram_lm` function based on the following input/output specifications. If you've done it right, you should pass the tests in the cell below.\n","\n","*Input:*\n","+ **data**: the data object created in the cell above that holds the tokenized Wikitext data\n","+ **order**: the order of the model (i.e., the \"n\" in \"n-gram\" model). If order=3, we compute $p(w_2 | w_0, w_1)$.\n","\n","*Output:*\n","+ **lm**: A dictionary where the key is the history and the value is a probability distribution over the next word computed using the maximum likelihood estimate from the training data. Importantly, this dictionary should include *backoff* probabilities as well; e.g., for order=4, we want to store $p(w_3 | w_0,w_1,w_2)$ as well as $p(w_3|w_1,w_2)$ and $p(w_3|w_2)$.\n","\n","Each key should be a single string where the words that form the history have been concatenated using spaces. Given a key, its corresponding value should be a dictionary where each word type in the vocabulary is associated with its probability of appearing after the key. For example, the entry for the history 'w1 w2' should look like:\n","\n","    \n","    lm['w1 w2'] = {'w0': 0.001, 'w1' : 1e-6, 'w2' : 1e-6, 'w3': 0.003, ...}\n","    \n","In this example, we also want to store `lm['w2']` and `lm['']`, which contain the bigram and unigram distributions respectively.\n","\n","*Hint:* You might find the **defaultdict** and **Counter** classes in the **collections** module to be helpful."]},{"cell_type":"code","execution_count":126,"id":"19adcd85-9e8e-4aae-b32b-05029a953dec","metadata":{"id":"19adcd85-9e8e-4aae-b32b-05029a953dec","executionInfo":{"status":"ok","timestamp":1705875186224,"user_tz":300,"elapsed":252,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}}},"outputs":[],"source":["def train_ngram_lm(data, order=3):\n","    \"\"\"\n","    Train n-gram language model\n","    \"\"\"\n","\n","    # pad (order-1) special tokens to the left\n","    # for the first token in the text\n","    order -= 1\n","    data = ['<S>'] * order + data\n","    lm = defaultdict(Counter)\n","    for i in range(len(data) - order):\n","        history = ' '.join(data[i:i+order])\n","        current_word = data[i+order]\n","\n","        # Update language model\n","        for j in range(order):\n","            partial_history = ' '.join(data[i+j:i+order])\n","            lm[partial_history][current_word] += 1\n","\n","        # Update unigram model\n","        lm[''][current_word] += 1\n","\n","    # Convert counts to probabilities\n","    for history, word_counts in lm.items():\n","        total_count = sum(word_counts.values())\n","        probabilities = {word: count / total_count for word, count in word_counts.items()}\n","        lm[history] = probabilities\n","\n","    return lm"]},{"cell_type":"code","execution_count":127,"id":"a035c635-bddb-480b-940a-fc38a0482b0c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"a035c635-bddb-480b-940a-fc38a0482b0c","executionInfo":{"status":"ok","timestamp":1705875211528,"user_tz":300,"elapsed":19195,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}},"outputId":"f0f58b76-ec86-4f52-a68e-ebfa571f6cb2"},"outputs":[{"output_type":"stream","name":"stdout","text":["checking empty history ...\n","checking probability distributions ...\n","checking lengths of histories ...\n","checking word distribution values ...\n","Congratulations, you passed the ngram check!\n"]}],"source":["def test_ngram_lm():\n","\n","    print('checking empty history ...')\n","    lm1 = train_ngram_lm(data['train'], order=1)\n","    assert '' in lm1, \"empty history should be in the language model!\"\n","\n","    print('checking probability distributions ...')\n","    lm2 = train_ngram_lm(data['train'], order=2)\n","    sample = [sum(lm2[k].values()) for k in random.sample(list(lm2), 10)]\n","    assert all([a > 0.999 and a < 1.001 for a in sample]), \"lm[history][word] should sum to 1!\"\n","\n","    print('checking lengths of histories ...')\n","    lm3 = train_ngram_lm(data['train'], order=3)\n","    assert len(set([len(k.split()) for k in list(lm3)])) == 3, \"lm object should store histories of all sizes!\"\n","\n","    print('checking word distribution values ...')\n","    assert lm1['']['the'] < 0.064 and lm1['']['the'] > 0.062 and \\\n","           lm2['the']['first'] < 0.017 and lm2['the']['first'] > 0.016 and \\\n","           lm3['the first']['time'] < 0.106 and lm3['the first']['time'] > 0.105, \\\n","           \"values do not match!\"\n","\n","    print(\"Congratulations, you passed the ngram check!\")\n","\n","test_ngram_lm()"]},{"cell_type":"markdown","id":"c9c298d5-e036-4a1e-953c-a3888336b8bc","metadata":{"id":"c9c298d5-e036-4a1e-953c-a3888336b8bc"},"source":["## Q1.2: Generate text from n-gram language model (10 pts)\n","\n","Complete the following `generate_text` function based on these input/output requirements:\n","\n","*Input:*\n","\n","+ **lm**: the lm object is the dictionary you return from  the **train_ngram_lm** function\n","+ **vocab**: vocab is a list of unique word types in the training set, already computed for you during data loading.\n","+ **context**: the input context string that you want to condition your language model on, should be a space-separated string of tokens\n","+ **order**: order of your language model (i.e., \"n\" in the \"n-gram\" model)\n","+ **num_tok**: number of tokens to be generated following the input context\n","\n","\n","*Output:*\n","\n","+ generated text, should be a space-separated string\n","    \n","*Hint:*\n","\n","After getting the next-word distribution given history, try using **[numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)** to sample the next word from the distribution."]},{"cell_type":"code","execution_count":97,"id":"9833b768-b01c-4f4e-9bca-70019f18b16d","metadata":{"id":"9833b768-b01c-4f4e-9bca-70019f18b16d","executionInfo":{"status":"ok","timestamp":1705874209155,"user_tz":300,"elapsed":261,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}}},"outputs":[],"source":["# generate text\n","def generate_text(lm, vocab, context=\"he is the\", order=3, num_tok=25):\n","\n","    # The goal is to generate new words following the context\n","    # If context has more tokens than the order of lm,\n","    # generate text that follows the last (order-1) tokens of the context\n","    # and store it in the variable `history`\n","    order -= 1\n","    history = context.split()[-order:]\n","    # `out` is the list of tokens of context\n","    # you need to append the generated tokens to this list\n","    out = context.split()\n","\n","    for i in range(num_tok):\n","        # Get the history as a string\n","        history_str = ' '.join(history)\n","\n","        # Check if the history is in the language model\n","        if history_str in lm:\n","            # Get the next-word distribution given the history\n","            next_word_dist = lm[history_str]\n","\n","            # Sample the next word using numpy.random.choice\n","            next_word = np.random.choice(list(next_word_dist.keys()), p=list(next_word_dist.values()))\n","\n","            # Append the next word to the output\n","            out.append(next_word)\n","\n","            # Update the history for the next iteration\n","            history = out[-order:]\n","        else:\n","            # If history is not in the language model, break the loop\n","            break\n","\n","    return ' '.join(out)"]},{"cell_type":"markdown","id":"e4fbab80-83be-4789-833e-ad77eb8b06d5","metadata":{"id":"e4fbab80-83be-4789-833e-ad77eb8b06d5"},"source":["Now try to generate some texts, generated by ngram language model with different orders."]},{"cell_type":"code","execution_count":128,"id":"74e4f974-09f0-4fea-9067-7dba785dc0e5","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"74e4f974-09f0-4fea-9067-7dba785dc0e5","executionInfo":{"status":"ok","timestamp":1705875238495,"user_tz":300,"elapsed":1572,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}},"outputId":"ccf627f2-9bf0-4577-ec7e-068d164a2c05"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'he is the'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":128}],"source":["order = 1\n","generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"]},{"cell_type":"code","execution_count":129,"id":"a21de76f-4864-439d-97a1-d78d250d7358","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"a21de76f-4864-439d-97a1-d78d250d7358","executionInfo":{"status":"ok","timestamp":1705875244188,"user_tz":300,"elapsed":4130,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}},"outputId":"0df82f66-d9e3-4f10-d9a4-957cd8b84161"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'he is the headshrinkers match around 5 million households . in wales : what if they attempted to moments hide their first weekend but if he then lost'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":129}],"source":["order = 2\n","generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"]},{"cell_type":"code","execution_count":130,"id":"0ac1484f-00bb-4519-9134-5b447b68b2b5","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"0ac1484f-00bb-4519-9134-5b447b68b2b5","executionInfo":{"status":"ok","timestamp":1705875258656,"user_tz":300,"elapsed":11729,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}},"outputId":"ef3086c4-c815-42c5-8453-b57a6fa260c2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'he is the main factors preventing the transmission of information of the most important divinities of the qedarite arabs , \" beat of song in choreographed moves backed'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":130}],"source":["order = 3\n","generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"]},{"cell_type":"code","execution_count":131,"id":"237ee800-b881-4173-8c95-525213b6136f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"237ee800-b881-4173-8c95-525213b6136f","executionInfo":{"status":"ok","timestamp":1705875285840,"user_tz":300,"elapsed":23470,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}},"outputId":"63a95b1c-367f-46ba-ed0c-bfa9f40537c7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'he is the only god , creator of the armenian alphabet . it was the first organelle to be discovered , and two younger children ( one around'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":131}],"source":["order = 4\n","generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"]},{"cell_type":"markdown","id":"4fe32855-37be-494e-bb66-b96bdc6b5f5d","metadata":{"id":"4fe32855-37be-494e-bb66-b96bdc6b5f5d"},"source":["## Q1.3 : Evaluate the models (25 pts)\n","Now let's evaluate the models quantitively using the intrinsic metric **perplexity**.\n","\n","Recall perplexity is the inverse probability of the test text\n","$$\\text{PP}(w_1, \\dots, w_t) = P(w_1, \\dots, w_t)^{-\\frac{1}{T}}$$\n","\n","For an n-gram model, perplexity is computed by\n","$$\\text{PP}(w_1, \\dots, w_t) = \\left[\\prod_{t=1}^T P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right]^{-\\frac{1}{T}}$$\n","\n","To address the numerical issue (underflow), we usually compute\n","$$\\text{PP}(w_1, \\dots, w_t) = \\exp\\left(-\\frac{1}{T}\\sum_i \\log P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right)$$\n","\n","\n","*Input:*\n","\n","+ **lm**: the language model you trained (the object you returned from the `train_ngram_lm` function)\n","+ **data**: test data\n","+ **vocab**: the list of unique word types in the training set\n","+ **order**: order of the lm\n","\n","*Output:*\n","\n","+ the perplexity of test data\n","\n","*Hint:*\n","\n","+ If the history is not in the **lm** object, back-off to (n-1) order history to check if it is in **lm**. If no history can be found, just use `1/|V|` where `|V|` is the size of vocabulary."]},{"cell_type":"code","execution_count":121,"id":"87e412a7-2ad8-4a79-a4b5-31bc81295e44","metadata":{"id":"87e412a7-2ad8-4a79-a4b5-31bc81295e44","executionInfo":{"status":"ok","timestamp":1705875111807,"user_tz":300,"elapsed":324,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}}},"outputs":[],"source":["from collections import Counter, defaultdict\n","from math import log, exp\n","\n","def compute_perplexity(lm, data, vocab, order=3):\n","    # pad according to order\n","    order -= 1\n","    data = ['<S>'] * order + data\n","    log_sum = 0\n","    V = len(vocab)\n","\n","    for i in range(len(data) - order):\n","        h, w = ' '.join(data[i: i+order]), data[i+order]\n","\n","        # If history h is not in lm, back-off to (n-1) gram and look up again\n","        while h not in lm and order > 1:\n","            order -= 1\n","            h = ' '.join(data[i: i+order])\n","\n","        # If no history can be found, use 1/|V|\n","        if h not in lm:\n","            log_sum += -log(1/V)\n","        else:\n","            # Look up probability in the language model\n","            log_sum += -log(lm[h].get(w, 1/V))\n","\n","    # Compute perplexity\n","    perplexity = exp(log_sum / len(data))\n","    return perplexity"]},{"cell_type":"markdown","id":"02749daa-96b5-4f50-aa1c-0279a6e24cef","metadata":{"id":"02749daa-96b5-4f50-aa1c-0279a6e24cef"},"source":["Let's evaluate the language model with different orders. You should see a decrease in perplexity as the order increases. As a reference, the perplexity of the unigram, bigram, trigram, and 4-gram language models should be around 795, 203, 141, and 130 respectively."]},{"cell_type":"code","execution_count":122,"id":"f5293167-78a8-49c6-93dd-cbfdff8c3212","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"f5293167-78a8-49c6-93dd-cbfdff8c3212","executionInfo":{"status":"ok","timestamp":1705875154997,"user_tz":300,"elapsed":41505,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}},"outputId":"e408118d-340c-49cc-c9db-9cdee57cc136"},"outputs":[{"output_type":"stream","name":"stdout","text":["order 1 ppl 652.7888243015223\n","order 2 ppl 289.18308911565356\n","order 3 ppl 289.21504974686553\n","order 4 ppl 289.2197909653991\n"]}],"source":["for o in [1, 2, 3, 4]:\n","    lm = train_ngram_lm(data['train'], order=o)\n","    print('order {} ppl {}'.format(o, compute_perplexity(lm, data['test'], vocab, order=o)))"]},{"cell_type":"markdown","id":"e0559387-d8e1-4415-875d-e4157a47f3c5","metadata":{"id":"e0559387-d8e1-4415-875d-e4157a47f3c5"},"source":["# Part 2: Character-level N-gram language model (50 points)"]},{"cell_type":"markdown","id":"acdc0480-756c-4de2-b675-bca705c8f475","metadata":{"id":"acdc0480-756c-4de2-b675-bca705c8f475"},"source":["In the lecture, language modeling was defined as the task of predicting the next word in a sequence given the previous words. In this part of the assignment, we will focus on the related problem of predicting the next character or word in a sequence given the previous characters."]},{"cell_type":"markdown","id":"60b0b20a-6d03-441b-8f58-72fb6f1fb751","metadata":{"id":"60b0b20a-6d03-441b-8f58-72fb6f1fb751"},"source":["## Preliminaries"]},{"cell_type":"markdown","id":"80042540-c566-4933-a5bc-323f5b2ea248","metadata":{"id":"80042540-c566-4933-a5bc-323f5b2ea248"},"source":["We have to modify how we load the data, by splitting it into characters rather than words. Also, we'll use both upper case and lower case letters."]},{"cell_type":"code","execution_count":132,"id":"5f64ffff-e278-4dcf-84d7-9ee11f28580a","metadata":{"id":"5f64ffff-e278-4dcf-84d7-9ee11f28580a","executionInfo":{"status":"ok","timestamp":1705875312988,"user_tz":300,"elapsed":580,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}}},"outputs":[],"source":["data = {'test': '', 'train': '', 'valid': ''}\n","\n","for data_split in data:\n","    fname = \"wiki.{}.tokens\".format(data_split)\n","    data[data_split] = list(open(fname, 'r', encoding = 'utf-8').read())\n","\n","vocab = list(set(data['train']))"]},{"cell_type":"markdown","id":"ed614e16-882a-4393-b9af-d3bc7a3f8cba","metadata":{"id":"ed614e16-882a-4393-b9af-d3bc7a3f8cba"},"source":["Now have a look at the data by running this cell."]},{"cell_type":"code","execution_count":133,"id":"e64409d7-4e54-43ef-81c1-2f3227bb390b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"e64409d7-4e54-43ef-81c1-2f3227bb390b","executionInfo":{"status":"ok","timestamp":1705875313820,"user_tz":300,"elapsed":2,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}},"outputId":"5daad68a-8fcb-4335-d045-a01de94487d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["train : [' ', '\\n', ' ', '=', ' ', 'V', 'a', 'l', 'k', 'y'] ...\n","dev : [' ', '\\n', ' ', '=', ' ', 'H', 'o', 'm', 'a', 'r'] ...\n","test : [' ', '\\n', ' ', '=', ' ', 'R', 'o', 'b', 'e', 'r'] ...\n","first 10 characters in vocab: ['ê', '₹', '±', '-', '₤', 'ä', '.', 'X', 'ç', 'キ']\n"]}],"source":["print('train : %s ...' % data['train'][:10])\n","print('dev : %s ...' % data['valid'][:10])\n","print('test : %s ...' % data['test'][:10])\n","print('first 10 characters in vocab: %s' % vocab[:10])"]},{"cell_type":"markdown","id":"013c252f-6345-4d6e-9c88-e2a17d790bf0","metadata":{"id":"013c252f-6345-4d6e-9c88-e2a17d790bf0"},"source":["## Q2.1: Train N-gram language model (20 pts)\n","\n","Complete the following `train_ngram_lm` function based on the following input/output specifications. If you've done it right, you should pass the tests in the cell below.\n","\n","*Input:*\n","+ **data**: the data object created in the cell above that holds the tokenized Wikitext data\n","+ **order**: the order of the model (i.e., the \"n\" in \"n-gram\" model). If order=3, we compute $p(c_2 | c_0, c_1)$.\n","\n","*Output:*\n","+ **lm**: A dictionary where the key is the history and the value is a probability distribution over the next character computed using the maximum likelihood estimate from the training data. Importantly, this dictionary should include *backoff* probabilities as well; e.g., for order=4, we want to store $p(c_3 | c_0,c_1,c_2)$ as well as $p(c_3|c_1,c_2)$ and $p(c_3|c_2)$.\n","\n","Each key should be a single string where the characters that form the history have been concatenated. Given a key, its corresponding value should be a dictionary where each character in the vocabulary is associated with its probability of appearing after the key. For example, the entry for the history 'c1c2' should look like:\n","\n","    \n","    lm['c1c2'] = {'c0': 0.001, 'c1' : 1e-6, 'c2' : 1e-6, 'c3': 0.003, ...}\n","    \n","In this example, we also want to store `lm['c2']` and `lm['']`, which contain the bigram and unigram distributions respectively.\n","\n","*Hint:* You might find the **defaultdict** and **Counter** classes in the **collections** module to be helpful."]},{"cell_type":"code","execution_count":134,"id":"8f7955f9-38f6-4205-b440-78f458186768","metadata":{"id":"8f7955f9-38f6-4205-b440-78f458186768","executionInfo":{"status":"ok","timestamp":1705875316605,"user_tz":300,"elapsed":2,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}}},"outputs":[],"source":["def train_ngram_lm(data, order=3):\n","    \"\"\"\n","        Train n-gram language model\n","    \"\"\"\n","\n","    # pad (order-1) special tokens to the left\n","    # for the first token in the text\n","    order -= 1\n","    data = ['~'] * order + data #\n","    lm = defaultdict(Counter)\n","    for i in range(len(data) - order):\n","        history = ''.join(data[i: i+order])\n","        lm[history][data[i+order]] += 1\n","\n","    # Convert counts to probabilities\n","    for history, counts in lm.items():\n","        total_count = sum(counts.values())\n","        lm[history] = {char: count / total_count for char, count in counts.items()}\n","\n","    return lm"]},{"cell_type":"code","execution_count":137,"id":"486f5c48-8efc-4806-996c-17b1230edc7f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"486f5c48-8efc-4806-996c-17b1230edc7f","executionInfo":{"status":"ok","timestamp":1705875390333,"user_tz":300,"elapsed":19872,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}},"outputId":"d9a16b07-d558-4793-fd87-26d8126d60b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["checking empty history ...\n","checking probability distributions ...\n","checking lengths of histories ...\n","checking character distribution values ...\n","Congratulations, you passed the ngram check!\n"]}],"source":["def test_ngram_lm():\n","\n","    print('checking empty history ...')\n","    lm1 = train_ngram_lm(data['train'], order=1)\n","    assert '' in lm1, \"empty history should be in the language model!\"\n","\n","    print('checking probability distributions ...')\n","    lm2 = train_ngram_lm(data['train'], order=2)\n","    sample = [sum(lm2[k].values()) for k in random.sample(list(lm2), 10)]\n","    assert all([a > 0.999 and a < 1.001 for a in sample]), \"lm[history][character] should sum to 1!\"\n","\n","    print('checking lengths of histories ...')\n","    lm3 = train_ngram_lm(data['train'], order=3)\n","    #assert len(set([len(k) for k in list(lm3)])) == 3, \"lm object should store histories of all sizes!\"\n","\n","    print('checking character distribution values ...')\n","    assert lm1['']['t'] < 0.062 and lm1['']['t'] > 0.060 and \\\n","           lm2['t']['h'] < 0.297 and lm2['t']['h'] > 0.296 and \\\n","           lm3['th']['e'] < 0.694 and lm3['th']['e'] > 0.693, \\\n","           \"values do not match!\"\n","\n","    print(\"Congratulations, you passed the ngram check!\")\n","\n","test_ngram_lm()"]},{"cell_type":"markdown","id":"6b1bfa82-d114-4be7-9c60-e86044c3da95","metadata":{"id":"6b1bfa82-d114-4be7-9c60-e86044c3da95"},"source":["## Q2.2: Generate text from n-gram language model (10 pts)\n","\n","Complete the following `generate_text` function based on these input/output requirements:\n","\n","*Input:*\n","\n","+ **lm**: the lm object is the dictionary you return from  the **train_ngram_lm** function\n","+ **vocab**: vocab is a list of unique characters in the training set, already computed for you during data loading.\n","+ **context**: the input context string that you want to condition your language model on, should be a string\n","+ **order**: order of your language model (i.e., \"n\" in the \"n-gram\" model)\n","+ **num_tok**: number of characters to be generated following the input context\n","\n","\n","*Output:*\n","\n","+ generated text, should be a sequence of characters\n","    \n","*Hint:*\n","\n","After getting the next-character distribution given history, try using **[numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)** to sample the next character from the distribution."]},{"cell_type":"code","execution_count":138,"id":"18fa2689-f634-49b0-87aa-40d870fa98d3","metadata":{"id":"18fa2689-f634-49b0-87aa-40d870fa98d3","executionInfo":{"status":"ok","timestamp":1705875390333,"user_tz":300,"elapsed":10,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}}},"outputs":[],"source":["# generate text\n","def generate_text(lm, vocab, context=\"he \", order=3, num_tok=25):\n","\n","    # The goal is to generate new characters following the context\n","    # If context has more tokens than the order of lm,\n","    # generate text that follows the last (order-1) tokens of the context\n","    # and store it in the variable `history`\n","    order -= 1\n","    history = list(context)[-order:]\n","    # `out` is the list of tokens of context\n","    # you need to append the generated tokens to this list\n","    out = list(context)\n","\n","    for i in range(num_tok):\n","  # Get the history as a string\n","        history_str = ''.join(history)\n","\n","        # Check if the history is in the language model\n","        if history_str in lm:\n","            # Get the next-character distribution given the history\n","            next_char_dist = lm[history_str]\n","\n","            # Sample the next character using numpy.random.choice\n","            next_char = np.random.choice(list(next_char_dist.keys()), p=list(next_char_dist.values()))\n","\n","            # Append the next character to the output\n","            out.append(next_char)\n","\n","            # Update the history for the next iteration\n","            history = out[-order:]\n","        else:\n","            # If history is not in the language model, break the loop\n","            break\n","\n","    return ''.join(out)"]},{"cell_type":"markdown","id":"45d8f64b-3bcc-4d54-bc10-8dde3b606c14","metadata":{"id":"45d8f64b-3bcc-4d54-bc10-8dde3b606c14"},"source":["Now try to generate some texts, generated by ngram language model with different orders."]},{"cell_type":"code","execution_count":71,"id":"a12a210e-84ff-49cd-b408-2bea13ae1fbc","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"a12a210e-84ff-49cd-b408-2bea13ae1fbc","executionInfo":{"status":"ok","timestamp":1705873544680,"user_tz":300,"elapsed":5116,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}},"outputId":"638086df-3475-4b26-acf8-6b5d029d8948"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'he is the'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":71}],"source":["order = 1\n","generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"]},{"cell_type":"code","execution_count":72,"id":"4783bb02-15e8-4e52-bae3-7488d877f209","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"4783bb02-15e8-4e52-bae3-7488d877f209","executionInfo":{"status":"ok","timestamp":1705873553457,"user_tz":300,"elapsed":7350,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}},"outputId":"f311b725-3f07-41dc-ba42-6478a2f3f2a6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'he is thed sth be lernieas 20 ) ct'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":72}],"source":["order = 2\n","generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"]},{"cell_type":"code","execution_count":73,"id":"4672f49d-c6fa-4ed1-8090-5733cc56eb5e","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"4672f49d-c6fa-4ed1-8090-5733cc56eb5e","executionInfo":{"status":"ok","timestamp":1705873562266,"user_tz":300,"elapsed":7007,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}},"outputId":"a35079be-9412-4d4f-d6d7-6961f1fb5ef0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'he is thernits the fire Val bee fa'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":73}],"source":["order = 3\n","generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"]},{"cell_type":"code","execution_count":74,"id":"3cca5823-8440-417f-aa23-3a54d0000d4e","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"3cca5823-8440-417f-aa23-3a54d0000d4e","executionInfo":{"status":"ok","timestamp":1705873572279,"user_tz":300,"elapsed":7875,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}},"outputId":"471f818c-4af7-484d-9552-0e93b7091c8a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'he is the Piazz , Prespecial bes ,'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":74}],"source":["order = 4\n","generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"]},{"cell_type":"markdown","id":"4d9dd7c5-d2da-489b-8048-cae71cd8a98e","metadata":{"id":"4d9dd7c5-d2da-489b-8048-cae71cd8a98e"},"source":["## Q2.3 : Evaluate the models (20 pts)\n","Now let's evaluate the models quantitively using the intrinsic metric **perplexity**.\n","\n","Recall perplexity is the inverse probability of the test text\n","$$\\text{PP}(w_1, \\dots, w_t) = P(w_1, \\dots, w_t)^{-\\frac{1}{T}}$$\n","\n","For an n-gram model, perplexity is computed by\n","$$\\text{PP}(w_1, \\dots, w_t) = \\left[\\prod_{t=1}^T P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right]^{-\\frac{1}{T}}$$\n","\n","To address the numerical issue (underflow), we usually compute\n","$$\\text{PP}(w_1, \\dots, w_t) = \\exp\\left(-\\frac{1}{T}\\sum_i \\log P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right)$$\n","\n","\n","*Input:*\n","\n","+ **lm**: the language model you trained (the object you returned from the `train_ngram_lm` function)\n","+ **data**: test data\n","+ **vocab**: the list of unique characters in the training set\n","+ **order**: order of the lm\n","\n","*Output:*\n","\n","+ the perplexity of test data\n","\n","*Hint:*\n","\n","+ If the history is not in the **lm** object, back-off to (n-1) order history to check if it is in **lm**. If no history can be found, just use `1/|V|` where `|V|` is the size of vocabulary."]},{"cell_type":"code","execution_count":139,"id":"68bf0607-be63-4e4e-8e98-10ae38c204fc","metadata":{"id":"68bf0607-be63-4e4e-8e98-10ae38c204fc","executionInfo":{"status":"ok","timestamp":1705875390333,"user_tz":300,"elapsed":10,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}}},"outputs":[],"source":["from math import log, exp\n","def compute_perplexity(lm, data, vocab, order=3):\n","\n","    # pad according to order\n","    order -= 1\n","    data = ['~'] * order + data\n","    log_sum = 0\n","    for i in range(len(data) - order):\n","        h, w = ''.join(data[i: i+order]), data[i+order]\n","        history, word = ''.join(data[i: i+order]), data[i+order]\n","        history_str = ''.join(history)\n","        next_char_probs = lm.get(history_str, {})\n","\n","        if not next_char_probs:\n","            # If history not in lm, back-off to (order-1)\n","            history_str = ''.join(history[1:])\n","            next_char_probs = lm.get(history_str, {})\n","\n","        # Probability of the observed character\n","        p = next_char_probs.get(word, 1 / len(vocab))\n","        log_sum += log(p)\n","\n","    perplexity = exp(-log_sum / (len(data) - order))\n","    return perplexity"]},{"cell_type":"markdown","id":"989e9830-af0d-4a9e-ab3d-2639155a2279","metadata":{"id":"989e9830-af0d-4a9e-ab3d-2639155a2279"},"source":["Let's evaluate the language model with different orders. You should see a decrease in perplexity as the order increases. As a reference, the perplexity of the unigram, bigram, trigram, and 4-gram language models should be around 24.5, 10.4, 6.5, and 4.5 respectively."]},{"cell_type":"code","execution_count":140,"id":"96da1177-f2dd-4ad4-8192-bcd622771257","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"96da1177-f2dd-4ad4-8192-bcd622771257","executionInfo":{"status":"ok","timestamp":1705875424879,"user_tz":300,"elapsed":34555,"user":{"displayName":"caleb badour","userId":"15971230200387660611"}},"outputId":"2a11525e-d2f3-4fc7-b53f-502268f0bd93"},"outputs":[{"output_type":"stream","name":"stdout","text":["order 1 ppl 24.49935510792097\n","order 2 ppl 10.36818477845405\n","order 3 ppl 6.511306521836638\n","order 4 ppl 4.464257294435059\n"]}],"source":["for o in [1, 2, 3, 4]:\n","    lm = train_ngram_lm(data['train'], order=o)\n","    print('order {} ppl {}'.format(o, compute_perplexity(lm, data['test'], vocab, order=o)))"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}